{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "approved-arabic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Starting the local TPU driver.\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': Not found: Unable to find driver in registry given worker: local://\n",
      "INFO:absl:Unable to initialize backend 'tpu': Invalid argument: TpuPlatform is not available.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import gc\n",
    "import logging\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "import math\n",
    "import json\n",
    "from flax.serialization import to_bytes, from_bytes\n",
    "\n",
    "import shutil\n",
    "import torch\n",
    "from transformers.file_utils import PushToHubMixin\n",
    "from datasets import load_metric\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize, GaussianBlur\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import transformers\n",
    "from flax import jax_utils, traverse_util\n",
    "from flax.jax_utils import unreplicate\n",
    "from flax.training import train_state\n",
    "from flax.training.common_utils import get_metrics, shard, shard_prng_key, get_metrics, onehot\n",
    "from flax_clip_vision_marian.modeling_clip_vision_marian import FlaxCLIPVisionMarianForConditionalGeneration\n",
    "from transformers import MarianTokenizer,MBart50TokenizerFast, HfArgumentParser, TrainingArguments, is_tensorboard_available, set_seed\n",
    "\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "                    Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n",
    "                    CenterCrop(image_size),\n",
    "                    ConvertImageDtype(torch.float),\n",
    "                    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "class ImageTextDataset(VisionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        file_path: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        max_samples: int = None\n",
    "    ):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "\n",
    "        # self.captions = []\n",
    "        # self.image_paths = []\n",
    "        # self.lang = []\n",
    "\n",
    "        examples = pd.read_csv(file_path, sep='\\t')\n",
    "        gc.collect()\n",
    "\n",
    "        self.map_lang_code = {\n",
    "            \"en\": \"en_XX\",\n",
    "            \"de\": \"de_DE\",\n",
    "            \"fr\": \"fr_XX\",\n",
    "            \"es\": \"es_XX\",\n",
    "        }\n",
    "\n",
    "        # for idx,img_file in enumerate(examples[\"image_file\"].values):\n",
    "        #     if os.path.exists(os.path.join(self.root,img_file)):\n",
    "        #     self.image_paths.append(img_file)\n",
    "        #     self.captions.append(examples[\"caption\"].values[idx])\n",
    "        #     self.lang.append(examples[\"lang_id\"].values[idx])\n",
    "\n",
    "        self.image_paths = examples[\"image_path\"].values\n",
    "        self.captions = examples[\"captions\"].values\n",
    "\n",
    "        if max_samples is None:\n",
    "            max_samples = len(self.image_paths)\n",
    "\n",
    "        self.image_paths = self.image_paths[:max_samples]\n",
    "        self.captions = self.captions[:max_samples]\n",
    "\n",
    "        # with open(file_path, encoding=\"utf-8\") as fd:\n",
    "        #     examples = csv.DictReader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "        #     for row in examples:\n",
    "        #         self.image_paths.append(os.path.join(self.root,row[\"image_file\"]))\n",
    "        #         self.captions.append(row[\"caption\"])\n",
    "        #         self.lang.append(row[\"lang_id\"])\n",
    "\n",
    "\n",
    "    def _load_image(self, idx: int):\n",
    "        path = self.image_paths[idx]\n",
    "        return read_image(os.path.join(self.root,path), mode=ImageReadMode.RGB)\n",
    "\n",
    "    def _load_target(self, idx):\n",
    "        return self.captions[idx]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image = self._load_image(index)\n",
    "        target = self._load_target(index)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "            \n",
    "        print(image)\n",
    "        return image, target,\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions)\n",
    "model = FlaxCLIPVisionMarianForConditionalGeneration.from_pretrained('munggok/image-captioning-marian')\n",
    "config = model.config\n",
    "preprocess = Transform(config.clip_vision_config.image_size)\n",
    "preprocess = torch.jit.script(preprocess)\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "focused-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = config.clip_vision_config.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "white-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torch.nn.Sequential(\n",
    "                    Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n",
    "                    CenterCrop(image_size),\n",
    "                    ConvertImageDtype(torch.float),\n",
    "                    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "welcome-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_trans = read_image('000000512774.jpg', mode=ImageReadMode.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "statutory-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_trans = transforms(read_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "favorite-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.stack([read_trans]).permute(0, 2, 3, 1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "early-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pixel_values = np.load('data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "varying-assurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True],\n",
       "         [ True,  True,  True]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img == pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distributed-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageTextDataset(\n",
    "    'data',\n",
    "    'data/dev.tsv',\n",
    "    transform=preprocess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broadband-planning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageTextDataset\n",
       "    Number of datapoints: 3\n",
       "    Root location: data\n",
       "    StandardTransform\n",
       "Transform: RecursiveScriptModule(\n",
       "             original_name=Transform\n",
       "             (transforms): RecursiveScriptModule(\n",
       "               original_name=Sequential\n",
       "               (0): RecursiveScriptModule(original_name=Resize)\n",
       "               (1): RecursiveScriptModule(original_name=CenterCrop)\n",
       "               (2): RecursiveScriptModule(original_name=ConvertImageDtype)\n",
       "               (3): RecursiveScriptModule(original_name=Normalize)\n",
       "             )\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "signed-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_val(examples):\n",
    "        pixel_values = torch.stack([example[0] for example in examples]).permute(0, 2, 3, 1).numpy()\n",
    "        captions = [example[1] for example in examples]\n",
    "\n",
    "        # tokenizer = map_tokenizer_lang[lang_id[0]]\n",
    "          # every validation loader has same language\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            tokens = tokenizer(captions, max_length=16, padding=\"max_length\", return_tensors=\"np\", truncation=True)\n",
    "\n",
    "        # had to create another enum of sorts for lang_id\n",
    "        # lang_id = np.array([map_lang_num[lang] for lang in lang_id])  # str of type <class 'numpy.ndarray'> is not a valid JAX type\n",
    "        decoder_input_ids = shift_tokens_right(tokens[\"input_ids\"], config.marian_config.pad_token_id)\n",
    "        print(captions)\n",
    "        batch = {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokens[\"input_ids\"],\n",
    "            \"attention_mask\": tokens[\"attention_mask\"],\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            # \"lang\": lang_id,\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "technological-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids: np.array, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = np.zeros(input_ids.shape, dtype=np.int64)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "    shifted_input_ids[:, 0] = pad_token_id\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "median-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "looking-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn_val,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "embedded-cambodia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6092,  1.7260,  1.1128,  ...,  1.9303,  1.9303,  1.9303],\n",
      "         [ 1.5800,  1.7990,  1.7114,  ...,  1.9303,  1.9303,  1.9303],\n",
      "         [ 1.6238,  1.5654,  1.8573,  ...,  1.9303,  1.9303,  1.9303],\n",
      "         ...,\n",
      "         [-0.8142, -1.0039, -0.9602,  ...,  0.4267,  0.4559,  0.4267],\n",
      "         [-0.5222, -0.7704, -1.0769,  ...,  0.4121,  0.3975,  0.2661],\n",
      "         [-0.6536, -0.9018, -0.9893,  ...,  0.4559,  0.3829,  0.1201]],\n",
      "\n",
      "        [[ 1.7747,  1.8498,  1.2945,  ...,  2.0749,  2.0749,  2.0749],\n",
      "         [ 1.8047,  1.9698,  1.9398,  ...,  2.0749,  2.0749,  2.0749],\n",
      "         [ 1.8047,  1.6547,  2.0299,  ...,  2.0749,  2.0749,  2.0749],\n",
      "         ...,\n",
      "         [-0.9117, -1.1218, -1.2869,  ..., -0.8967, -0.8816, -0.9267],\n",
      "         [-0.3864, -0.7616, -1.0918,  ..., -0.9417, -0.8816, -0.9717],\n",
      "         [-0.7466, -1.1368, -1.0467,  ..., -0.9117, -0.8666, -1.0167]],\n",
      "\n",
      "        [[ 1.7762,  1.7904,  1.2358,  ...,  2.1459,  2.1459,  2.1459],\n",
      "         [ 1.8331,  1.9610,  1.8615,  ...,  2.1459,  2.1459,  2.1459],\n",
      "         [ 1.7477,  1.6624,  1.9895,  ...,  2.1459,  2.1459,  2.1459],\n",
      "         ...,\n",
      "         [-0.9114, -1.1105, -1.2100,  ..., -1.2954, -1.2100, -1.1674],\n",
      "         [-0.3426, -0.6555, -0.9967,  ..., -1.3238, -1.2243, -1.2527],\n",
      "         [-0.8119, -1.0394, -0.9967,  ..., -1.2669, -1.2669, -1.4091]]])\n",
      "['seseorang melihat orang lain berdiri di dalam air']\n",
      "{'pixel_values': array([[[[ 1.6091708 ,  1.7747284 ,  1.7761754 ],\n",
      "         [ 1.7259582 ,  1.8497672 ,  1.7903955 ],\n",
      "         [ 1.1128243 ,  1.2944798 ,  1.2358129 ],\n",
      "         ...,\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ],\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ],\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ]],\n",
      "\n",
      "        [[ 1.5799739 ,  1.8047439 ,  1.8330555 ],\n",
      "         [ 1.7989503 ,  1.9698293 ,  1.9610363 ],\n",
      "         [ 1.7113597 ,  1.9398139 ,  1.8614956 ],\n",
      "         ...,\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ],\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ],\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ]],\n",
      "\n",
      "        [[ 1.6237692 ,  1.8047439 ,  1.7477353 ],\n",
      "         [ 1.5653756 ,  1.6546663 ,  1.6624149 ],\n",
      "         [ 1.857344  ,  2.0298605 ,  1.9894764 ],\n",
      "         ...,\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ],\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ],\n",
      "         [ 1.9303361 ,  2.0748837 ,  2.145897  ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8141679 , -0.9116621 , -0.91141707],\n",
      "         [-1.0039475 , -1.1217709 , -1.1104981 ],\n",
      "         [-0.96015227, -1.2868563 , -1.2100385 ],\n",
      "         ...,\n",
      "         [ 0.42669836, -0.8966543 , -1.2953589 ],\n",
      "         [ 0.4558952 , -0.8816466 , -1.2100385 ],\n",
      "         [ 0.42669836, -0.92666984, -1.1673783 ]],\n",
      "\n",
      "        [[-0.5221994 , -0.38639018, -0.34261447],\n",
      "         [-0.7703726 , -0.76158434, -0.65545595],\n",
      "         [-1.0769397 , -1.0917554 , -0.9967375 ],\n",
      "         ...,\n",
      "         [ 0.41209993, -0.94167763, -1.3237991 ],\n",
      "         [ 0.3975015 , -0.8816466 , -1.2242587 ],\n",
      "         [ 0.26611567, -0.9716932 , -1.2526988 ]],\n",
      "\n",
      "        [[-0.65358526, -0.7465766 , -0.81187665],\n",
      "         [-0.9017585 , -1.1367786 , -1.0393977 ],\n",
      "         [-0.9893491 , -1.0467321 , -0.9967375 ],\n",
      "         ...,\n",
      "         [ 0.4558952 , -0.9116621 , -1.2669188 ],\n",
      "         [ 0.38290307, -0.8666388 , -1.2669188 ],\n",
      "         [ 0.12013142, -1.0167165 , -1.4091195 ]]]], dtype=float32), 'input_ids': array([[  512,   233,    43,   214,  1457,    19,    82,   478,     0,\n",
      "        54795, 54795, 54795, 54795, 54795, 54795, 54795]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]), 'decoder_input_ids': array([[54795,   512,   233,    43,   214,  1457,    19,    82,   478,\n",
      "            0, 54795, 54795, 54795, 54795, 54795, 54795]])}\n",
      "tensor([[[-1.2959, -1.3397, -1.2959,  ..., -0.9310, -0.9893, -1.0039],\n",
      "         [-1.3105, -1.3251, -1.3397,  ..., -0.9748, -1.0477, -1.0623],\n",
      "         [-1.3251, -1.3105, -1.3689,  ..., -1.0039, -1.1061, -1.0915],\n",
      "         ...,\n",
      "         [-1.0769, -0.8434, -0.9164,  ..., -0.2740, -0.3178, -0.3616],\n",
      "         [-1.0623, -0.8288, -0.8580,  ..., -0.2302, -0.2740, -0.5514],\n",
      "         [-0.9893, -0.9456, -0.9748,  ..., -0.3178, -0.5368, -0.2156]],\n",
      "\n",
      "        [[-0.4014, -0.3264, -0.2213,  ...,  0.3640,  0.3940,  0.3340],\n",
      "         [-0.4164, -0.3264, -0.2213,  ...,  0.3490,  0.3940,  0.3040],\n",
      "         [-0.4314, -0.2963, -0.2513,  ...,  0.3190,  0.3490,  0.2589],\n",
      "         ...,\n",
      "         [-1.2118, -1.1818, -1.1968,  ..., -0.9867, -0.9867, -0.8967],\n",
      "         [-1.1968, -1.1368, -1.0918,  ..., -0.8967, -0.9417, -1.1218],\n",
      "         [-1.1218, -1.2568, -1.1818,  ..., -0.8666, -1.0767, -0.7466]],\n",
      "\n",
      "        [[-0.5417, -0.5559, -0.3568,  ...,  1.0510,  1.0936,  0.8945],\n",
      "         [-0.5559, -0.5701, -0.3995,  ...,  1.0367,  1.1078,  0.8519],\n",
      "         [-0.5701, -0.5559, -0.4279,  ...,  1.0083,  1.0936,  0.8377],\n",
      "         ...,\n",
      "         [-1.0394, -1.0252, -1.0821,  ..., -1.0821, -1.0394, -0.8403],\n",
      "         [-1.0110, -0.9825, -0.9825,  ..., -0.9967, -0.9683, -1.0678],\n",
      "         [-0.9541, -1.0963, -1.0394,  ..., -0.9256, -1.0394, -0.7266]]])\n",
      "['satu pemain bisbol menabrak pemain lain selama pertandingan']\n",
      "{'pixel_values': array([[[[-1.2959161 , -0.40139794, -0.54169536],\n",
      "         [-1.3397113 , -0.32635912, -0.5559154 ],\n",
      "         [-1.2959161 , -0.22130474, -0.35683453],\n",
      "         ...,\n",
      "         [-0.9309554 ,  0.3639983 ,  1.0509521 ],\n",
      "         [-0.9893491 ,  0.39401382,  1.0936122 ],\n",
      "         [-1.0039475 ,  0.33398274,  0.8945313 ]],\n",
      "\n",
      "        [[-1.3105145 , -0.4164057 , -0.5559154 ],\n",
      "         [-1.3251129 , -0.32635912, -0.57013553],\n",
      "         [-1.3397113 , -0.22130474, -0.39949474],\n",
      "         ...,\n",
      "         [-0.9747507 ,  0.34899053,  1.036732  ],\n",
      "         [-1.0477428 ,  0.39401382,  1.1078323 ],\n",
      "         [-1.0623412 ,  0.3039672 ,  0.85187113]],\n",
      "\n",
      "        [[-1.3251129 , -0.43141347, -0.57013553],\n",
      "         [-1.3105145 , -0.29634356, -0.5559154 ],\n",
      "         [-1.3689082 , -0.25132027, -0.42793486],\n",
      "         ...,\n",
      "         [-1.0039475 ,  0.31897497,  1.0082918 ],\n",
      "         [-1.1061366 ,  0.34899053,  1.0936122 ],\n",
      "         [-1.0915381 ,  0.25894392,  0.8376511 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0769397 , -1.2118175 , -1.0393977 ],\n",
      "         [-0.8433648 , -1.1818019 , -1.0251776 ],\n",
      "         [-0.9163569 , -1.1968098 , -1.082058  ],\n",
      "         ...,\n",
      "         [-0.2740262 , -0.98670095, -1.082058  ],\n",
      "         [-0.31782144, -0.98670095, -1.0393977 ],\n",
      "         [-0.36161673, -0.8966543 , -0.8403168 ]],\n",
      "\n",
      "        [[-1.0623412 , -1.1968098 , -1.0109575 ],\n",
      "         [-0.82876635, -1.1367786 , -0.9825174 ],\n",
      "         [-0.8579632 , -1.0917554 , -0.9825174 ],\n",
      "         ...,\n",
      "         [-0.2302309 , -0.8966543 , -0.9967375 ],\n",
      "         [-0.2740262 , -0.94167763, -0.96829736],\n",
      "         [-0.55139625, -1.1217709 , -1.067838  ]],\n",
      "\n",
      "        [[-0.9893491 , -1.1217709 , -0.95407724],\n",
      "         [-0.94555384, -1.2568408 , -1.0962781 ],\n",
      "         [-0.9747507 , -1.1818019 , -1.0393977 ],\n",
      "         ...,\n",
      "         [-0.31782144, -0.8666388 , -0.9256371 ],\n",
      "         [-0.5367978 , -1.0767475 , -1.0393977 ],\n",
      "         [-0.21563247, -0.7465766 , -0.7265563 ]]]], dtype=float32), 'input_ids': array([[  191,  4667, 18255,  8993,  4667,   214,   484,  4309,     0,\n",
      "        54795, 54795, 54795, 54795, 54795, 54795, 54795]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]), 'decoder_input_ids': array([[54795,   191,  4667, 18255,  8993,  4667,   214,   484,  4309,\n",
      "            0, 54795, 54795, 54795, 54795, 54795, 54795]])}\n",
      "tensor([[[-1.1061e+00, -7.8497e-01, -4.0451e-02,  ...,  6.6027e-01,\n",
      "           6.4567e-01,  6.7487e-01],\n",
      "         [-9.0176e-01, -8.2877e-01,  1.0553e-01,  ...,  6.7487e-01,\n",
      "           5.7268e-01,  4.8509e-01],\n",
      "         [-7.1198e-01, -5.6599e-01, -1.4264e-01,  ...,  4.2670e-01,\n",
      "           4.1210e-01,  4.9969e-01],\n",
      "         ...,\n",
      "         [ 8.9385e-01,  7.0407e-01,  2.8071e-01,  ...,  1.3172e+00,\n",
      "           1.3610e+00,  1.3610e+00],\n",
      "         [ 5.1429e-01, -4.2001e-01,  5.2889e-01,  ...,  1.2296e+00,\n",
      "           1.2588e+00,  1.2880e+00],\n",
      "         [ 5.2889e-01,  4.1210e-01,  7.4786e-01,  ...,  1.2442e+00,\n",
      "           1.2442e+00,  1.2880e+00]],\n",
      "\n",
      "        [[-1.0918e+00, -9.7169e-01, -4.0140e-01,  ...,  1.6890e-01,\n",
      "           1.0887e-01,  1.5389e-01],\n",
      "         [-1.0918e+00, -1.1518e+00, -2.8134e-01,  ...,  1.8391e-01,\n",
      "           7.8851e-02, -1.1196e-02],\n",
      "         [-1.1668e+00, -9.8670e-01, -5.5148e-01,  ..., -4.1212e-02,\n",
      "          -5.6219e-02,  3.3827e-02],\n",
      "         ...,\n",
      "         [ 5.2908e-01,  3.3398e-01, -1.0124e-01,  ...,  4.8406e-01,\n",
      "           4.6905e-01,  4.8406e-01],\n",
      "         [ 1.6890e-01, -7.7659e-01,  1.9891e-01,  ...,  4.2403e-01,\n",
      "           4.2403e-01,  4.5404e-01],\n",
      "         [ 2.1392e-01,  7.8851e-02,  4.2403e-01,  ...,  4.5404e-01,\n",
      "           4.3904e-01,  4.8406e-01]],\n",
      "\n",
      "        [[-1.0678e+00, -1.0394e+00, -6.6968e-01,  ..., -4.2793e-01,\n",
      "          -4.1371e-01, -3.8527e-01],\n",
      "         [-1.0110e+00, -1.1532e+00, -5.1326e-01,  ..., -3.7105e-01,\n",
      "          -4.4215e-01, -5.2748e-01],\n",
      "         [-1.1674e+00, -1.0536e+00, -7.9766e-01,  ..., -5.1326e-01,\n",
      "          -5.2748e-01, -4.4215e-01],\n",
      "         ...,\n",
      "         [ 1.2665e-01, -8.6653e-02, -4.9904e-01,  ...,  1.2887e-02,\n",
      "           1.2887e-02,  2.7107e-02],\n",
      "         [-2.1463e-01, -1.1247e+00, -2.2885e-01,  ..., -4.3993e-02,\n",
      "          -2.9773e-02, -1.3329e-03],\n",
      "         [-1.7197e-01, -2.9995e-01, -1.5553e-02,  ..., -2.9773e-02,\n",
      "           1.2887e-02,  5.5547e-02]]])\n",
      "['sebuah kereta berhenti di sebuah stasiun tran dengan dua orang di peron terdekat']\n",
      "{'pixel_values': array([[[[-1.1061366e+00, -1.0917554e+00, -1.0678380e+00],\n",
      "         [-7.8497106e-01, -9.7169322e-01, -1.0393977e+00],\n",
      "         [-4.0451370e-02, -4.0139794e-01, -6.6967601e-01],\n",
      "         ...,\n",
      "         [ 6.6027313e-01,  1.6889732e-01, -4.2793486e-01],\n",
      "         [ 6.4567471e-01,  1.0886613e-01, -4.1371480e-01],\n",
      "         [ 6.7487156e-01,  1.5388943e-01, -3.8527465e-01]],\n",
      "\n",
      "        [[-9.0175849e-01, -1.0917554e+00, -1.0109575e+00],\n",
      "         [-8.2876635e-01, -1.1517864e+00, -1.1531583e+00],\n",
      "         [ 1.0553299e-01, -2.8133580e-01, -5.1325524e-01],\n",
      "         ...,\n",
      "         [ 6.7487156e-01,  1.8390508e-01, -3.7105459e-01],\n",
      "         [ 5.7268262e-01,  7.8850597e-02, -4.4215491e-01],\n",
      "         [ 4.8509204e-01, -1.1196004e-02, -5.2747530e-01]],\n",
      "\n",
      "        [[-7.1197891e-01, -1.1667942e+00, -1.1673783e+00],\n",
      "         [-5.6599468e-01, -9.8670095e-01, -1.0536178e+00],\n",
      "         [-1.4264035e-01, -5.5147564e-01, -7.9765660e-01],\n",
      "         ...,\n",
      "         [ 4.2669836e-01, -4.1211538e-02, -5.1325524e-01],\n",
      "         [ 4.1209993e-01, -5.6219306e-02, -5.2747530e-01],\n",
      "         [ 4.9969047e-01,  3.3827297e-02, -4.4215491e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 8.9384794e-01,  5.2908373e-01,  1.2664770e-01],\n",
      "         [ 7.0406842e-01,  3.3398274e-01, -8.6653285e-02],\n",
      "         [ 2.8071409e-01, -1.0124261e-01, -4.9903518e-01],\n",
      "         ...,\n",
      "         [ 1.3172023e+00,  4.8406041e-01,  1.2887171e-02],\n",
      "         [ 1.3609976e+00,  4.6905264e-01,  1.2887171e-02],\n",
      "         [ 1.3609976e+00,  4.8406041e-01,  2.7107235e-02]],\n",
      "\n",
      "        [[ 5.1428890e-01,  1.6889732e-01, -2.1463387e-01],\n",
      "         [-4.2001042e-01, -7.7659214e-01, -1.1247182e+00],\n",
      "         [ 5.2888733e-01,  1.9891284e-01, -2.2885394e-01],\n",
      "         ...,\n",
      "         [ 1.2296118e+00,  4.2402935e-01, -4.3993089e-02],\n",
      "         [ 1.2588086e+00,  4.2402935e-01, -2.9773025e-02],\n",
      "         [ 1.2880055e+00,  4.5404488e-01, -1.3328948e-03]],\n",
      "\n",
      "        [[ 5.2888733e-01,  2.1392061e-01, -1.7197368e-01],\n",
      "         [ 4.1209993e-01,  7.8850597e-02, -2.9995427e-01],\n",
      "         [ 7.4786371e-01,  4.2402935e-01, -1.5552960e-02],\n",
      "         ...,\n",
      "         [ 1.2442101e+00,  4.5404488e-01, -2.9773025e-02],\n",
      "         [ 1.2442101e+00,  4.3903711e-01,  1.2887171e-02],\n",
      "         [ 1.2880055e+00,  4.8406041e-01,  5.5547368e-02]]]],\n",
      "      dtype=float32), 'input_ids': array([[  347,  2539,   766,    19,   347,  5462, 40797,    41,   403,\n",
      "           43,    19,   981,   781,  7521,     0, 54795]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'decoder_input_ids': array([[54795,   347,  2539,   766,    19,   347,  5462, 40797,    41,\n",
      "          403,    43,    19,   981,   781,  7521,     0]])}\n"
     ]
    }
   ],
   "source": [
    "for asa in eval_loader:\n",
    "    print(asa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-movie",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
